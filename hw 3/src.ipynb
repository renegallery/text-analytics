{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "    code_show=true; \n",
       "    function code_toggle() {\n",
       "     if (code_show){\n",
       "     $('div.input').hide();\n",
       "     } else {\n",
       "     $('div.input').show();\n",
       "     }\n",
       "     code_show = !code_show\n",
       "    } \n",
       "    $( document ).ready(code_toggle);\n",
       "    </script>\n",
       "    The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "    To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "    code_show=true; \n",
    "    function code_toggle() {\n",
    "     if (code_show){\n",
    "     $('div.input').hide();\n",
    "     } else {\n",
    "     $('div.input').show();\n",
    "     }\n",
    "     code_show = !code_show\n",
    "    } \n",
    "    $( document ).ready(code_toggle);\n",
    "    </script>\n",
    "    The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "    To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document similarity\n",
    "## 0. Splitting, tokenization, normalization.\n",
    "Splitting: \n",
    "1. using regex to split the class bio into seperate files\n",
    "2. store in q1/split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "f = codecs.open('q0/classbios_unicode.txt','r', encoding = 'utf-8', errors = 'replace')\n",
    "text = f.read()\n",
    "text = text.encode('ascii','replace')\n",
    "# remove all the lines with \"<== ... ==>\"\n",
    "import re\n",
    "files = text.split(\"==> \")\n",
    "file_list = []\n",
    "i = 0\n",
    "for bio in files:\n",
    "    sp = bio.split(\" <==\")\n",
    "    name = sp[0]\n",
    "    try:\n",
    "        if name != \"?\":\n",
    "            file_list.append({name: sp[1]})\n",
    "            open( 'q0/split/'+ str(i) + '. ' + name + '.txt', \"w\").write(sp[1])\n",
    "            i += 1\n",
    "    except IndexError:\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tokenization and normalization: \n",
    "1. using java (lucence standard tokenizer and english analyzer), java class: NormalizeBio.java\n",
    "2. stored in q1/norm\n",
    "\n",
    "Stopwords removing:\n",
    "1. using python\n",
    "2. stored in q1/filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "stopword = stopwords.words('english')\n",
    "for filename in os.listdir('q0/norm/'):\n",
    "    with open('q0/norm/'+filename, 'r') as f:\n",
    "        content = f.read().split()\n",
    "        filt = [word for word in content if word not in stopword]\n",
    "        open('q0/filter/'+filename, \"w\").write(' '.join(filt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Boolean Similarity\n",
    "1. read in vocabulary in q0/filter document by document, stored into a list\n",
    "2. for each document, set the intersect to 1, set the rest to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "voc = []\n",
    "for filename in os.listdir('q0/filter/'):\n",
    "    with open('q0/filter/' + filename, 'r') as f:\n",
    "        if filename=='.DS_Store':\n",
    "            continue\n",
    "        voc += f.read().split()\n",
    "\n",
    "voc_set = list(set(voc))\n",
    "file_list = []\n",
    "filename_list = []\n",
    "for filename in os.listdir('q0/filter/'):\n",
    "    with open('q0/filter/' + filename, 'r') as f:\n",
    "        if filename=='.DS_Store':\n",
    "            continue\n",
    "        # set a word dictionary for f\n",
    "        word_dict = dict.fromkeys(voc_set, 0)\n",
    "        # counter\n",
    "        for word in f.read().split():\n",
    "            if word_dict[word]==0:\n",
    "                word_dict[word]=1\n",
    "        # transform into series\n",
    "        word_series = pd.Series(word_dict, name=filename)\n",
    "        file_list.append(word_series)\n",
    "        \n",
    "# concatenate series into dataframe\n",
    "term_doc_matrix = pd.concat(file_list, axis = 1)\n",
    "term_doc_matrix = term_doc_matrix.reindex_axis(sorted(term_doc_matrix.columns,\n",
    "                                                      key=lambda string: int(string.split(\".\")[0])), \n",
    "                                               axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boolean similarity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "def sim(series1, series2):\n",
    "    norm1, norm2 = LA.norm(series1), LA.norm(series2)\n",
    "    dot_prod = series1.dot(series2)\n",
    "    return round(dot_prod/(norm1*norm2),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate boolean similiarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = len(term_doc_matrix.columns)\n",
    "bool_sim_mat = np.zeros([n,n])\n",
    "for i in range(n):\n",
    "    for j in range(i,n):\n",
    "        bool_sim_mat[i, j] = sim(term_doc_matrix.ix[:,i], term_doc_matrix.ix[:,j])\n",
    "for i in range(n):\n",
    "    for j in range(i):\n",
    "        bool_sim_mat[i, j] = bool_sim_mat[j, i]\n",
    "np.savetxt('q1/boolean.txt', bool_sim_mat, fmt='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_n_max(matrix, n):\n",
    "    M = 200000\n",
    "    top3 = [-M]*n\n",
    "    top3_ind = [(-1,-1)]*n\n",
    "    n, m = matrix.shape[0], matrix.shape[1]\n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            if matrix[i,j] > min(top3):\n",
    "                ind = top3.index(min(top3))\n",
    "                top3[ind] = matrix[i,j]\n",
    "                top3_ind[ind] = (i,j)\n",
    "    return zip(top3, top3_ind)\n",
    "def find_n_min(matrix, n):\n",
    "    M = 200000\n",
    "    bottom3 = [M]*n\n",
    "    bottom3_ind = [(-1,-1)]*n\n",
    "    n, m = matrix.shape[0], matrix.shape[1]\n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            if matrix[i,j] < max(bottom3):\n",
    "                ind = bottom3.index(max(bottom3))\n",
    "                bottom3[ind] = matrix[i,j]\n",
    "                bottom3_ind[ind] = (i,j)\n",
    "    return zip(bottom3, bottom3_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So finding the largest and smallest 3 number in the boolean similiarity matrix, the results are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0.33000000000000002, (27, 14)),\n",
       "  (0.33000000000000002, (14, 7)),\n",
       "  (0.33000000000000002, (34, 23)),\n",
       "  (0.33000000000000002, (35, 6))],\n",
       " [(0.070000000000000007, (33, 8)),\n",
       "  (0.11, (33, 18)),\n",
       "  (0.11, (18, 1)),\n",
       "  (0.10000000000000001, (15, 10))])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top, bottom = find_n_max(bool_sim_mat, 4), find_n_min(bool_sim_mat, 4)\n",
    "top, bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOP 3(4)\n",
    "Translating into people's names\n",
    "because these 4 pairs have same boolean similiarity, they are the \"top 3 pair\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27. Kate Overstreet.txt 14. Mengshan Jin.txt\n",
      "14. Mengshan Jin.txt 7. Mingyan Fan.txt\n",
      "34. Apurvaa Subramaniam.txt 23. Melissa McNeill.txt\n",
      "35. Brian Trent.txt 6. Alyssa Everding.txt\n"
     ]
    }
   ],
   "source": [
    "names = term_doc_matrix.columns\n",
    "for pair in top:\n",
    "    print names[pair[1][0]], names[pair[1][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOTTOM 3(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33. Alexander Stec.txt 8. Theodore Feder.txt\n",
      "15. Li Kan.txt 10. Jessica Feuston.txt\n",
      "33. Alexander Stec.txt 18. Yilei Li.txt\n",
      "18. Yilei Li.txt 1. Edward Binkley.txt\n"
     ]
    }
   ],
   "source": [
    "bottom = sorted(bottom, key = lambda pair: pair[0])\n",
    "for pair in bottom:\n",
    "    print names[pair[1][0]], names[pair[1][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. tf-idf Similiarity\n",
    "1. make a tf-idf matrix\n",
    "2. calculate similiarity matrix\n",
    "3. get top 3/bottom 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_doc(row):\n",
    "    counter = 0\n",
    "    for doc in row:\n",
    "        if doc>0:\n",
    "            counter+=1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "voc = []\n",
    "for filename in os.listdir('q0/filter/'):\n",
    "    with open('q0/filter/' + filename, 'r') as f:\n",
    "        if filename=='.DS_Store':\n",
    "            continue\n",
    "        voc += f.read().split()\n",
    "\n",
    "voc_set = list(set(voc))\n",
    "voc_dict = dict.fromkeys(voc_set, 0)\n",
    "file_list = []\n",
    "for filename in os.listdir('q0/filter/'):\n",
    "    with open('q0/filter/' + filename, 'r') as f:\n",
    "        if filename=='.DS_Store':\n",
    "            continue\n",
    "        # set a word dictionary for f\n",
    "        word_dict = dict.fromkeys(voc_set, 0)\n",
    "        # counter\n",
    "        for word in f.read().split():\n",
    "            word_dict[word]+=1\n",
    "            voc\n",
    "        # transform into series\n",
    "        word_series = pd.Series(word_dict, name=filename)\n",
    "        file_list.append(word_series)\n",
    "        \n",
    "# tf of every doc, term pair:\n",
    "term_doc_matrix = pd.concat(file_list, axis = 1)\n",
    "term_doc_matrix = term_doc_matrix.reindex_axis(sorted(term_doc_matrix.columns,\n",
    "                                                      key=lambda string: int(string.split(\".\")[0])), \n",
    "                                               axis=1)\n",
    "# number of documents:\n",
    "N = len(term_doc_matrix.columns)\n",
    "\n",
    "# number of voc:\n",
    "m = len(voc_set)\n",
    "\n",
    "# df of all the terms\n",
    "df_list = list(term_doc_matrix.apply(count_doc, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term_doc_mat = term_doc_matrix.as_matrix()\n",
    "term_doc_tfidf = np.zeros([m, N])\n",
    "\n",
    "for i in range(m):\n",
    "    for j in range(N):\n",
    "        if term_doc_mat[i, j]==0:\n",
    "            term_doc_tfidf[i, j]=0\n",
    "        else:\n",
    "            term_doc_tfidf[i, j] = (1+math.log(term_doc_mat[i, j]))*math.log(N/df_list[i])\n",
    "#np.savetxt('q2/term_doc_tfidf.txt',term_doc_tfidf, fmt='%.2f')\n",
    "term_doc_tfidf = pd.DataFrame(term_doc_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tfidf_sim_mat = np.zeros([N,N])\n",
    "for i in range(N):\n",
    "    for j in range(i,N):\n",
    "        tfidf_sim_mat[i, j] = sim(term_doc_tfidf.ix[:,i], term_doc_tfidf.ix[:,j])\n",
    "for i in range(n):\n",
    "    for j in range(i):\n",
    "        tfidf_sim_mat[i, j] = tfidf_sim_mat[j, i]\n",
    "np.savetxt('q2/tf_idf.txt', tfidf_sim_mat, fmt='%.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So finding the largest and smallest 3 number in the boolean similiarity matrix, the results are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0.11, (17, 7)), (0.12, (15, 2)), (0.11, (20, 7))],\n",
       " [(0.0, (15, 10)), (0.01, (3, 0)), (0.01, (5, 0))])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top, bottom = find_n_max(tfidf_sim_mat, 3), find_n_min(tfidf_sim_mat, 3)\n",
    "top, bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOP 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15. Li Kan.txt 2. Daniel Breitbach.txt\n",
      "17. Yungjen Kung.txt 7. Mingyan Fan.txt\n",
      "20. Eric Lundquist.txt 7. Mingyan Fan.txt\n"
     ]
    }
   ],
   "source": [
    "names = term_doc_matrix.columns\n",
    "top = sorted(top, key = lambda pair: -pair[0])\n",
    "for pair in top:\n",
    "    print names[pair[1][0]], names[pair[1][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOTTOM 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15. Li Kan.txt 10. Jessica Feuston.txt\n",
      "3. Valentinos Constantinou.txt 0. Kapil Vinay Bhatt.txt\n",
      "5. Yunyan Duan.txt 0. Kapil Vinay Bhatt.txt\n"
     ]
    }
   ],
   "source": [
    "bottom = sorted(bottom, key = lambda pair: pair[0])\n",
    "for pair in bottom:\n",
    "    print names[pair[1][0]], names[pair[1][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Indexing and retrieval\n",
    "## 3. Indexing\n",
    "1. download webpages(in Python) with urllib to request url, beautiful soup to read in contents\n",
    "2. creating Lucene index; add name and text as fields (in java) source file is in: *\"/Users/Rene/Documents/msia/16 Fall/text/hw 3/LuceneFirstApplication/src/com/tutorialspoint/lucene\"*, main function is LuceneTester.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "wikiprefix = \"https://en.wikipedia.org\"\n",
    "two_cap_country = ['State of Palestine', 'Sahrawi Arab Democratic Republic', 'Malaysia', 'Swaziland', 'Montserrat',\n",
    "                  'Montenegro', 'Benin', 'Sri Lanka', 'Bolivia', 'Georgia', \"Côte d'Ivoire\"]\n",
    "three_cap_country = ['South Africa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "f = urllib.urlopen(\"https://en.wikipedia.org/wiki/List_of_national_capitals_in_alphabetical_order\")\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(f.read())\n",
    "index_page = soup.find_all(\"table\")[2].find_all(\"tr\")[1:]\n",
    "\n",
    "cc_list = []\n",
    "for country in index_page:\n",
    "    if len(country.find_all(\"a\"))>2 and country.find_all(\"a\")[2].contents[0] in two_cap_country:\n",
    "        e1 = {'city_name': country.find_all(\"a\")[0].contents[0], \n",
    "             'city_href': wikiprefix + country.find_all(\"a\")[0]['href'], \n",
    "             'country_name': country.find_all(\"a\")[2].contents[0], \n",
    "             'country_href': wikiprefix + country.find_all(\"a\")[2]['href']}\n",
    "        cc_list.append(e1)\n",
    "        e2 = {'city_name': country.find_all(\"a\")[1].contents[0], \n",
    "             'city_href': wikiprefix + country.find_all(\"a\")[1]['href'], \n",
    "             'country_name': country.find_all(\"a\")[2].contents[0], \n",
    "             'country_href': wikiprefix + country.find_all(\"a\")[2]['href']}\n",
    "        cc_list.append(e2)\n",
    "    elif len(country.find_all(\"a\"))>3 and country.find_all(\"a\")[3].contents[0] in three_cap_country:\n",
    "        e1 = {'city_name': country.find_all(\"a\")[0].contents[0], \n",
    "             'city_href': wikiprefix + country.find_all(\"a\")[0]['href'], \n",
    "             'country_name': country.find_all(\"a\")[3].contents[0], \n",
    "             'country_href': wikiprefix + country.find_all(\"a\")[3]['href']}\n",
    "        cc_list.append(e1)\n",
    "        e2 = {'city_name': country.find_all(\"a\")[1].contents[0], \n",
    "             'city_href': wikiprefix + country.find_all(\"a\")[1]['href'], \n",
    "             'country_name': country.find_all(\"a\")[3].contents[0], \n",
    "             'country_href': wikiprefix + country.find_all(\"a\")[3]['href']}\n",
    "        cc_list.append(e2)\n",
    "        e3 = {'city_name': country.find_all(\"a\")[2].contents[0], \n",
    "             'city_href': wikiprefix + country.find_all(\"a\")[2]['href'], \n",
    "             'country_name': country.find_all(\"a\")[3].contents[0], \n",
    "             'country_href': wikiprefix + country.find_all(\"a\")[3]['href']}\n",
    "        cc_list.append(e3)\n",
    "    else:\n",
    "        e = {'city_name': country.find_all(\"a\")[0].contents[0], \n",
    "             'city_href': wikiprefix + country.find_all(\"a\")[0]['href'], \n",
    "             'country_name': country.find_all(\"a\")[1].contents[0], \n",
    "             'country_href': wikiprefix + country.find_all(\"a\")[1]['href']}\n",
    "        cc_list.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_text(link, tag_id):\n",
    "    f = urllib.urlopen(link)\n",
    "    soup = BeautifulSoup(f.read())\n",
    "    #print link\n",
    "    return soup.find(id = tag_id).text\n",
    "#read_text(\"https://en.wikipedia.org/wiki/Alofi\", \"mw-content-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "i = 0\n",
    "\n",
    "for country in cc_list:\n",
    "    info = {'city_name': country['city_name'], \n",
    "            'country_name': country['country_name'],\n",
    "            'city_text': read_text(country['city_href'], \"mw-content-text\"),\n",
    "            'country_text': read_text(country['country_href'], \"mw-content-text\")}\n",
    "    with open('/Users/Rene/Documents/msia/16 Fall/text/hw 3/q3/Data/' + str(i)+'. '+ info['city_name'] +'.json', 'w') as fp:\n",
    "        json.dump(info, fp)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrieval\n",
    "\n",
    "Java code in */Users/Rene/Documents/msia/16 Fall/text/hw 3/LuceneFirstApplication/src/com/tutorialspoint/lucene/queryer*\n",
    "### ‘Greek’ and ‘Roman’ but not ‘Persian’: \n",
    "Use a BooleanQuery for this. In addition to the Lucene docs, you might find this page a useful reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tripoli\n",
      "\n",
      "Tunis\n",
      "\n",
      "Sukhumi\n",
      "\n",
      "Nicosia\n",
      "\n",
      "Nicosia\n",
      "\n",
      "Tiraspol\n",
      "\n",
      "Monaco\n",
      "\n",
      "Lisbon\n",
      "\n",
      "Ramallah\n",
      "\n",
      "Bucharest\n",
      "\n",
      "Cairo\n",
      "\n",
      "Sofia\n",
      "\n",
      "Ljubljana\n",
      "\n",
      "Tirana\n",
      "\n",
      "Budapest\n",
      "\n",
      "Bangui\n",
      "\n",
      "Montevideo\n",
      "\n",
      "Skopje\n",
      "\n",
      "Belgrade\n",
      "\n",
      "Algiers\n",
      "\n",
      "Madrid\n",
      "\n",
      "Bern\n",
      "\n",
      "Berlin\n",
      "\n",
      "Gibraltar\n",
      "\n",
      "Bratislava\n",
      "\n",
      "Vilnius\n",
      "\n",
      "Dublin\n",
      "\n",
      "Amsterdam\n",
      "\n",
      "Kiev\n",
      "\n",
      "Warsaw\n",
      "\n",
      "Wellington\n",
      "\n",
      "Buenos Aires\n",
      "\n",
      "Copenhagen\n",
      "\n",
      "Havana\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/Rene/Documents/msia/16 Fall/text/hw 3/q4/booleanQuery.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‘Shakespeare’\n",
    "even if it’s misspelled: Use a FuzzyQuery for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London\n",
      "\n",
      "Prague\n",
      "\n",
      "Cairo\n",
      "\n",
      "Washington, D.C.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/Rene/Documents/msia/16 Fall/text/hw 3/q4/fuzzyQuery.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the words ‘located below sea level’ near each other\n",
    "Use a PhraseQuery with a slop factor of 10. In addition to the Lucene docs, you might find this page a useful reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baku\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/Rene/Documents/msia/16 Fall/text/hw 3/q4/phraseQuery.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### an interesting query of your choice: \n",
    "all city containing \"china\" in their city text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beijing\n",
      "\n",
      "Nanjing\n",
      "\n",
      "Hong Kong\n",
      "\n",
      "Taipei\n",
      "\n",
      "Vientiane\n",
      "\n",
      "Dushanbe\n",
      "\n",
      "Bandar Seri Begawan\n",
      "\n",
      "Singapore\n",
      "\n",
      "Episkopi Cantonment\n",
      "\n",
      "Bangkok\n",
      "\n",
      "Phnom Penh\n",
      "\n",
      "Pyongyang\n",
      "\n",
      "Dili\n",
      "\n",
      "Ulaanbaatar\n",
      "\n",
      "Bishkek\n",
      "\n",
      "Saipan\n",
      "\n",
      "Kathmandu\n",
      "\n",
      "Manila\n",
      "\n",
      "Tashkent\n",
      "\n",
      "Luanda\n",
      "\n",
      "Ashgabat\n",
      "\n",
      "Nouakchott\n",
      "\n",
      "Putrajaya\n",
      "\n",
      "Malé\n",
      "\n",
      "Sri Jayawardenepura Kotte\n",
      "\n",
      "Seoul\n",
      "\n",
      "Lilongwe\n",
      "\n",
      "Mogadishu\n",
      "\n",
      "Manama\n",
      "\n",
      "Muscat\n",
      "\n",
      "Astana\n",
      "\n",
      "Ramallah\n",
      "\n",
      "Naypyidaw\n",
      "\n",
      "Port Louis\n",
      "\n",
      "Sana'a\n",
      "\n",
      "Sukhumi\n",
      "\n",
      "Tokyo\n",
      "\n",
      "Port Vila\n",
      "\n",
      "Kutaisi\n",
      "\n",
      "Belmopan\n",
      "\n",
      "Bissau\n",
      "\n",
      "Nicosia\n",
      "\n",
      "Nicosia\n",
      "\n",
      "Hanoi\n",
      "\n",
      "Islamabad\n",
      "\n",
      "Kuwait City\n",
      "\n",
      "Riyadh\n",
      "\n",
      "Kampala\n",
      "\n",
      "Port Moresby\n",
      "\n",
      "Praia\n",
      "\n",
      "San José\n",
      "\n",
      "Suva\n",
      "\n",
      "Havana\n",
      "\n",
      "Jakarta\n",
      "\n",
      "New Delhi\n",
      "\n",
      "Baghdad\n",
      "\n",
      "Kabul\n",
      "\n",
      "Abu Dhabi\n",
      "\n",
      "Ankara\n",
      "\n",
      "Tehran\n",
      "\n",
      "Thimphu\n",
      "\n",
      "Beirut\n",
      "\n",
      "Cairo\n",
      "\n",
      "Damascus\n",
      "\n",
      "Doha\n",
      "\n",
      "Amman\n",
      "\n",
      "Castries\n",
      "\n",
      "Bamako\n",
      "\n",
      "Addis Ababa\n",
      "\n",
      "Kuala Lumpur\n",
      "\n",
      "Montevideo\n",
      "\n",
      "Budapest\n",
      "\n",
      "Canberra\n",
      "\n",
      "Paramaribo\n",
      "\n",
      "Hagåtña\n",
      "\n",
      "Jerusalem\n",
      "\n",
      "Lisbon\n",
      "\n",
      "Prague\n",
      "\n",
      "Baku\n",
      "\n",
      "Dhaka\n",
      "\n",
      "Lomé\n",
      "\n",
      "Lusaka\n",
      "\n",
      "Ouagadougou\n",
      "\n",
      "Papeete\n",
      "\n",
      "Roseau\n",
      "\n",
      "São Tomé\n",
      "\n",
      "St. George's\n",
      "\n",
      "Brazzaville\n",
      "\n",
      "Conakry\n",
      "\n",
      "Antananarivo\n",
      "\n",
      "Minsk\n",
      "\n",
      "Riga\n",
      "\n",
      "Algiers\n",
      "\n",
      "Guatemala City\n",
      "\n",
      "Khartoum\n",
      "\n",
      "Nassau\n",
      "\n",
      "Nuuk\n",
      "\n",
      "Rabat\n",
      "\n",
      "Buenos Aires\n",
      "\n",
      "Nairobi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/Rene/Documents/msia/16 Fall/text/hw 3/q4/myQuery.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LDA\n",
    "1. use java to create a term-docment-freq list (in *'/Users/Rene/Documents/msia/16 Fall/text/hw 3/LuceneFirstApplication/src/com/tutorialspoint/lucene/LDA.java'*)\n",
    "2. read into python and pivot the table into matrix\n",
    "3. run lda with lda library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "termDoc = pd.read_csv('/Users/Rene/Documents/msia/16 Fall/text/hw 3/q5/termDoc.txt', header = None, \n",
    "                      delimiter = '|')\n",
    "termDoc.columns = ['doc', 'term', 'freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "termDoc1 = termDoc.drop_duplicates(subset = ['doc','term'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filtering terms that are english word, removing digit and special marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = '[^\\W\\d]'\n",
    "#termDoc2 = termDoc1[re.search(pattern, termDoc1['term'], flags = re.IGNORECASE) is not None]\n",
    "#termDoc1.filter([1], regex = pattern, axis = 0)\n",
    "termDoc2_1 = termDoc1[termDoc1['term'].str.contains(r'^[a-zA-Z0-9?><;,{}[\\]\\-_+=!@#$%\\^&*|\\']*$', \n",
    "                                                    flags=re.IGNORECASE, regex=True, na=False)]\n",
    "termDoc2 = termDoc1[termDoc1['term'].str.contains(r'[^\\W\\d]', flags=re.IGNORECASE, regex=True, na=False)]\n",
    "termDoc3 = termDoc2_1[~termDoc2_1['term'].str.contains(r'[_.\\'0-9]', flags=re.IGNORECASE, regex=True, na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Document Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term_doc_matrix = pd.pivot_table(termDoc3, index = 'term', columns = 'doc', values = 'freq', fill_value = 0)\n",
    "#term_doc_matrix.to_csv('/Users/Rene/Documents/msia/16 Fall/text/hw 3/q5/termDoc_matrix.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term_doc_matrix1 = term_doc_matrix.as_matrix().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lda\n",
    "import lda.datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA instance at 0x1133b2e18>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = termDoc3['term'].unique().tolist()\n",
    "title = termDoc3['doc'].unique().tolist()\n",
    "model = lda.LDA(n_topics=50, n_iter=1000, random_state = 1)\n",
    "model.fit(term_doc_matrix1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check model convergency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1176d3310>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKJQsOq/QcegAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ccd6810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# skipping the first few entries makes the graph more readable\n",
    "plt.plot(model.loglikelihoods_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_word = model.topic_word_\n",
    "n_top_words = 10\n",
    "topics = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    if not topics:\n",
    "        topics = ['Topic {}: {}'.format(i, ' '.join(topic_words))]\n",
    "    else:\n",
    "        topics = topics + ['Topic {}: {}'.format(i, ' '.join(topic_words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic = model.doc_topic_\n",
    "topic_map = {}\n",
    "topic_map = topic_map.fromkeys(range(50))\n",
    "for i in range(len(title)):\n",
    "    topic = doc_topic[i].argmax()\n",
    "    if topic_map[topic] is None:\n",
    "        topic_map[topic] = [title[i]]\n",
    "    else:\n",
    "        topic_map[topic].append(title[i])\n",
    "    #print(\"{} (top topic: {})\".format(title[i], topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 frequent topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Topic 0: otomi seringat chamanga pannonhalma hormouz rai sarnia terror pase',\n",
       " 'Topic 22: comercio sarnia ofek irreligi sukhothai pujehun isaf upeneu bertold',\n",
       " 'Topic 10: impos sept immin sephardim sopemi lic hemmerl dispersi domenichini',\n",
       " 'Topic 38: comercio valpovo breadfield perciv wrai aka jaga koisan erkend',\n",
       " 'Topic 45: lic concacaf poper meighoo valpovo uliss comercio ratliff kra',\n",
       " 'Topic 49: comercio perciv zisser centru codi valpovo biosystem ovenston highschool',\n",
       " 'Topic 1: bugti hydrocarbon seringat ministerstvem hormouz door sombolac kosmin vitacress',\n",
       " 'Topic 18: guang machu angelokastro hertslet immin izabrao rove gct impos',\n",
       " 'Topic 37: poper snif alchemi essi chilembw residu tavira meighoo wikisourc',\n",
       " 'Topic 19: kurmi charca nass kherdba nunaminer arirang macouba chibchan posit']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "topic_len = {}\n",
    "topic_len = topic_len.fromkeys(range(50))\n",
    "for key in topic_len.keys():\n",
    "    if topic_map[key] is not None:\n",
    "        topic_len[key] = len(topic_map[key])\n",
    "    else:\n",
    "        topic_len[key] = 0\n",
    "top10_topic_number = sorted(topic_len.iteritems(), key=operator.itemgetter(1), reverse=True)[:10]\n",
    "top10_topic = [topics[topic_num[0]] for topic_num in top10_topic_number]\n",
    "top10_topic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
