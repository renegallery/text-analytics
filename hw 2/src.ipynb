{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "    code_show=true; \n",
       "    function code_toggle() {\n",
       "     if (code_show){\n",
       "     $('div.input').hide();\n",
       "     } else {\n",
       "     $('div.input').show();\n",
       "     }\n",
       "     code_show = !code_show\n",
       "    } \n",
       "    $( document ).ready(code_toggle);\n",
       "    </script>\n",
       "    The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "    To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "    code_show=true; \n",
    "    function code_toggle() {\n",
    "     if (code_show){\n",
    "     $('div.input').hide();\n",
    "     } else {\n",
    "     $('div.input').show();\n",
    "     }\n",
    "     code_show = !code_show\n",
    "    } \n",
    "    $( document ).ready(code_toggle);\n",
    "    </script>\n",
    "    The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "    To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenization\n",
    "\n",
    "## Analysis\n",
    "1. CoreNLP seperates and takes punctuation as tokens, including comma, dollar sign, quotation, double dash\n",
    "2. For single dash, CoreNLP doesn't divide the words with single dash as seperate words, like buy-back. But Lucene Standard tokenizer will divide it into words.\n",
    "\n",
    "\n",
    "**CODE IN JAVA CLASS \"Tokenization\", RESULT ARE SHOWED IN q1.txt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Normalization\n",
    "\n",
    "## Analysis\n",
    "\n",
    "### lemmatizer and stemmer\n",
    "1. CoreNLP lemmatizer doesn't remove punctuations while stemmers do.\n",
    "2. CoreNLP lemmatizer does not change the plural of nouns(words vs word) or case(we vs us) or tempus(said vs say) or comparison(easy vs easiest)\n",
    "3. CoreNLP lemmatizer does not change the capital of proper nouns.\n",
    "4. CoreNLP changes \"is/are\" into \"be\"\n",
    "\n",
    "### different stemmers:\n",
    "1. KStemFilter chops punctuations and dollar signs.\n",
    "2. PorterFilter transforms words by removing or replacing suffix(temporarily->temporarili)\n",
    "3. EnglishFilter transforms uppercase into lowercase and filter out some initial stopwords, like \"is\", \"the\"\n",
    "\n",
    "**CODE IN JAVA CLASS \"Normalization\", RESULT ARE SHOWED IN q2.txt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Class Bio \n",
    "## File preprocessing\n",
    "1. **removing \"<== ... ==>\"**\n",
    "2. **split sentences with nlp ssplit**\n",
    "\n",
    "## Tokenization and Normalization\n",
    "I used Lucene's StandardTokenizer and EnglishAnalyzer for this purpose. The reason of choosing them is below:\n",
    "1. The StandardTokenizer is grammer based so it will tokenize and split sentence more reasonable. \n",
    "2. English Anaylzer not only filter some initial stopwords but also normalize it based on english grammer. \n",
    "\n",
    "**RESULTS ARE STORED IN classbio_norm.txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing CoreNLP and request from API\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in the txt file\n",
    "# encode the txt file as utf-8\n",
    "#f = open('classbios_unicode.txt','r')\n",
    "#text = f.read()\n",
    "import codecs\n",
    "f = codecs.open('classbios_unicode.txt','r', encoding = 'utf-8', errors = 'replace')\n",
    "text = f.read()\n",
    "text = text.encode('ascii','replace')\n",
    "# remove all the lines with \"<== ... ==>\"\n",
    "import re\n",
    "text1 = re.sub(r'==>[\\w\\s]*<==', '\\n', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split sentences\n",
    "# use annotator 'ssplit' for splitting\n",
    "ssplit = nlp.annotate(text1,properties={'annotators':'ssplit', 'outputFormat':'json'})\n",
    "# number of sentences: 657\n",
    "s_len = len(ssplit['sentences'])\n",
    "# write to file \n",
    "target = open('classbio_clean.txt','w')\n",
    "# join the tokens from sentences\n",
    "for i in range(s_len):\n",
    "    sentence = [t['word'] for t in ssplit['sentences'][i]['tokens']]\n",
    "    target.write(' '.join(sentence[:-1])+'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Basic frequency analysis. \n",
    "After classbio is tokenized and normalized in java with Standard Tokenizer and English Analyzer, it is stored as classbio_norm.txt. Read it back into python and start frequency analysis in the following steps:\n",
    "1. **Filter out stopwords**: a list of initial stop words has been filtered out by Lucene English Analyzer but it's not adequent, so I import stopwords list from nltk corpus and did some further filtering.\n",
    "2. **Word Count dictionary construction**: word and their count are stored as a dictionary.\n",
    "3. **Dict into Data Frame and sort**: get top 20 by sorting values.\n",
    "\n",
    "## Analysis\n",
    "The top 20 word list gives a reasonable amount of information about the corpus and their interests and background in analytics and since a lot of \"analyt\" and \"analysi\" show up. Although our general interest is in data science, \"data\" did not show up on top, instead, \"science\" appears. \"work\" and \"compani\" also come on top, expressing general background about work experience and real world application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>analyt</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>work</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>text</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>interest</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>program</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>us</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>year</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>learn</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>scienc</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>graduat</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>project</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>like</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>msia</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>compani</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>analysi</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>appli</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>want</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>also</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>univers</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  count\n",
       "723     analyt    137\n",
       "76        work    102\n",
       "1604      text     96\n",
       "290   interest     70\n",
       "938    program     64\n",
       "1357        us     58\n",
       "623       year     55\n",
       "1021     learn     54\n",
       "1682    scienc     51\n",
       "585    graduat     48\n",
       "192    project     48\n",
       "1729      like     46\n",
       "1094      msia     39\n",
       "1013   compani     36\n",
       "408    analysi     35\n",
       "1449     appli     34\n",
       "84        want     34\n",
       "1151      also     34\n",
       "529    univers     34"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use java to normalize the class bio\n",
    "# read in the normalized file\n",
    "f = open('classbio_norm.txt','r')\n",
    "norm = f.read()\n",
    "word_count = {}\n",
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "stopword = stopwords.words('english')\n",
    "for word in norm.split():\n",
    "    if word not in stopword:\n",
    "        if word not in word_count:\n",
    "            word_count[word] = 1\n",
    "        else:\n",
    "            word_count[word]+=1\n",
    "import pandas as pd\n",
    "word_count_pd = pd.DataFrame(word_count.items(), columns=['word','count'])\n",
    "# a list of initial stop words has been filtered out by Lucene English Analyzer\n",
    "# removing stop words further by using stopwords in nltk package:\n",
    "word_count_pd.sort_values('count', ascending=False)[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Bigram Frequency Analysis\n",
    "The same counting process is repeated. \n",
    "## Analysis\n",
    "This time, things start making more sense. \"data scienc\" and \"data scientist\" shows up on top, some terminology of data science, like \"unstructur data\" and \"text data\" as well as \"machine learning\". Some time points and locations like \"new york\" and \"high school\" indicates people's background and their ways of explaining it.\n",
    "This is more informative than the single word frequency results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>data scienc</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>msia program</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4059</th>\n",
       "      <td>interest text</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>data scientist</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>machin learn</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4769</th>\n",
       "      <td>unstructur data</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5747</th>\n",
       "      <td>text data</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2753</th>\n",
       "      <td>new york</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>data analysi</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321</th>\n",
       "      <td>becam interest</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>comput scienc</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5258</th>\n",
       "      <td>join msia</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3288</th>\n",
       "      <td>high school</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1876</th>\n",
       "      <td>data analyt</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>work data</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>data visual</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3475</th>\n",
       "      <td>scienc analyt</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4949</th>\n",
       "      <td>program northwestern</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>york citi</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    bigram  count\n",
       "1045           data scienc     31\n",
       "175           msia program     21\n",
       "4059         interest text     20\n",
       "3095        data scientist     15\n",
       "736           machin learn     15\n",
       "4769       unstructur data     13\n",
       "5747             text data     12\n",
       "2753              new york     11\n",
       "531           data analysi     10\n",
       "2321        becam interest     10\n",
       "1457         comput scienc      9\n",
       "5258             join msia      8\n",
       "3288           high school      8\n",
       "1876           data analyt      8\n",
       "3200             work data      8\n",
       "4249           data visual      8\n",
       "3475         scienc analyt      8\n",
       "4949  program northwestern      8\n",
       "939              york citi      7"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "prword = \"\"\n",
    "bigram_count = {}\n",
    "for word in norm.split():\n",
    "    if word not in stopword:\n",
    "        if prword is not \"\":\n",
    "            bigram = prword+\" \"+word\n",
    "            if bigram not in bigram_count:\n",
    "                bigram_count[bigram] = 1\n",
    "            else:\n",
    "                bigram_count[bigram]+=1\n",
    "        prword = word\n",
    "bigram_count_pd = pd.DataFrame(bigram_count.items(), columns=['bigram','count'])\n",
    "bigram_count_pd.sort_values('count', ascending=False)[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sentiment Analysis\n",
    "## 1. baseline naive bayes classifier\n",
    "Classifier: \n",
    "P(class|words) = P(class, words)/P(words) =P(class)*P(words|class)/P(words) \n",
    "\n",
    "P(pos) = P(neg) = number of documents in each class/total number of documents\n",
    "\n",
    "So only compare P(words|class), under naive bayes assumption of independency, it is equivalent to calculate P(word1|class)\\*...\\*P(wordn|class), also equivalent to sum of log(P(word1|class)).\n",
    "\n",
    "process: \n",
    "1. build voc, count words in each class\n",
    "2. Laplace smoothing log prob\n",
    "3. For a test review, sum of log and do a comparison\n",
    "\n",
    "## Analysis\n",
    "I used a test review from the positive review. With the classifier, it successfully classify it as positive with a higher sum of log probability as outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in pos and neg:\n",
    "import os\n",
    "import math\n",
    "pos, neg = \"\", \"\"\n",
    "pos_path = 'review_polarity/txt_norm/pos/'\n",
    "neg_path = 'review_polarity/txt_norm/neg/'\n",
    "   \n",
    "for filename in os.listdir(pos_path):\n",
    "    with open(pos_path+filename, 'r') as f:\n",
    "        content = f.read()\n",
    "    pos+=content\n",
    "for filename in os.listdir(neg_path):\n",
    "    with open(neg_path+filename, 'r') as f:\n",
    "        content = f.read()\n",
    "    neg+=content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "def get_text(path, namestr):\n",
    "    text = \"\"\n",
    "    for filename in glob.glob(os.path.join(path, namestr)):\n",
    "        with open(filename, 'r') as f:\n",
    "            content = f.read()\n",
    "        text+=content\n",
    "    return text\n",
    "def get_textlist(path, namestr):\n",
    "    textlist = []\n",
    "    for filename in glob.glob(os.path.join(path, namestr)):\n",
    "        with open(filename, 'r') as f:\n",
    "            content = f.read()\n",
    "        textlist.append(content)\n",
    "    return textlist\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopword = stopwords.words('english')\n",
    "import pandas as pd\n",
    "\n",
    "def build_voc(pos, neg):\n",
    "    pos_voc, neg_voc = set(pos.split()), set(neg.split())\n",
    "    from nltk.corpus import stopwords\n",
    "    stopword = stopwords.words('english')\n",
    "    return list((pos_voc | neg_voc)-set(stopword))\n",
    "\n",
    "def word_count(review, voc):\n",
    "    word_count = {}\n",
    "    for word in voc:\n",
    "        word_count[word]=0    \n",
    "    for word in review.split():\n",
    "        if word not in stopword:\n",
    "            word_count[word]+=1\n",
    "    return word_count\n",
    "\n",
    "def naive_bayes_train(pos, neg):\n",
    "    voc = build_voc(pos, neg)\n",
    "    word_count_pos, word_count_neg = word_count(pos, voc), word_count(neg, voc)\n",
    "    word_count_pos_pd = pd.DataFrame(word_count_pos.items(), columns=['word','count'])\n",
    "    word_count_neg_pd = pd.DataFrame(word_count_neg.items(), columns=['word','count'])\n",
    "    # log(P(word|pos)):\n",
    "    total_count_pos = sum(word_count_pos_pd['count']+1)\n",
    "    word_count_pos_pd['logprob'] = word_count_pos_pd.apply(lambda row: math.log((row['count']+1.0)/total_count_pos), axis = 1)\n",
    "    # log(P(word|neg))f:\n",
    "    total_count_neg = sum(word_count_neg_pd['count']+1)\n",
    "    word_count_neg_pd['logprob'] = word_count_neg_pd.apply(lambda row: math.log((row['count']+1.0)/total_count_neg), axis = 1)\n",
    "    return {'pos': word_count_pos_pd, 'neg': word_count_neg_pd}\n",
    "\n",
    "def naive_bayes_test(train_dict, test):\n",
    "    word_count_pos_pd = train_dict['pos']\n",
    "    word_count_neg_pd = train_dict['neg']\n",
    "    pos_prob = sum(word_count_pos_pd[word_count_pos_pd['word'].isin(test)]['logprob'])\n",
    "    neg_prob = sum(word_count_neg_pd[word_count_neg_pd['word'].isin(test)]['logprob'])\n",
    "    if (pos_prob>neg_prob):\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"negative\"\n",
    "\n",
    "def naive_bayes_validation(train_dict, testlist, actual_class):\n",
    "    predict = [naive_bayes_test(train_dict, test.split()) for test in testlist]\n",
    "    try:\n",
    "        combo = zip(predict,actual_class)\n",
    "        count = Counter(combo)\n",
    "        try:\n",
    "            precision = count[('positive','positive')]/float(count[('positive','negative')]+count[('positive','positive')])\n",
    "            recall = count[('positive','positive')]/float(count[('negative','positive')]+count[('positive','positive')])\n",
    "            Fscore = 2.0*precision*recall/(precision+recall)\n",
    "            print \"precision \"+str(precision)+\", recall \"+str(recall)+\", Fscore \"+str(Fscore)\n",
    "            return (precision, recall, Fscore)\n",
    "        except ValueError:\n",
    "            print \"no positive prediction or no positive class\"\n",
    "            print count\n",
    "    except IndexError:\n",
    "        print \"predict and actual not the same length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dict = naive_bayes_train(pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test review\n",
    "with open('review_polarity/txt_norm/pos/cv000_29590.txt', 'r') as f:\n",
    "    test = f.read().split()\n",
    "naive_bayes_test(train_dict, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Evaluation\n",
    "### train on cv0 and test on cv6&7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in pos and neg:\n",
    "import os\n",
    "import math\n",
    "pos_path = 'review_polarity/txt_norm/pos/'\n",
    "neg_path = 'review_polarity/txt_norm/neg/'\n",
    "\n",
    "pos_training = get_text(pos_path, \"cv0*.txt\")\n",
    "neg_training = get_text(neg_path, \"cv0*.txt\")\n",
    "train_dict = naive_bayes_train(pos_training, neg_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision 0.8, recall 0.76, Fscore 0.779487179487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8, 0.76, 0.7794871794871796)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textlist = get_textlist(pos_path, \"cv[67]*.txt\")+get_textlist(neg_path, \"cv[67]*.txt\")\n",
    "actual_class = ['positive']*200 + ['negative']*200\n",
    "result = naive_bayes_validation(train_dict, textlist, actual_class)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train on cv0&1&2 and test on cv6&7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision 0.835978835979, recall 0.79, Fscore 0.81233933162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8359788359788359, 0.79, 0.8123393316195374)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in pos and neg:\n",
    "import os\n",
    "import math\n",
    "pos_path = 'review_polarity/txt_norm/pos/'\n",
    "neg_path = 'review_polarity/txt_norm/neg/'\n",
    "\n",
    "pos_training = get_text(pos_path, \"cv[012]*.txt\")\n",
    "neg_training = get_text(neg_path, \"cv[012]*.txt\")\n",
    "train_dict = naive_bayes_train(pos_training, neg_training)\n",
    "\n",
    "textlist = get_textlist(pos_path, \"cv[67]*.txt\")+get_textlist(neg_path, \"cv[67]*.txt\")\n",
    "actual_class = ['positive']*200 + ['negative']*200\n",
    "result = naive_bayes_validation(train_dict, textlist, actual_class)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train on cv0&1&2&3&4 and test on cv6&7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision 0.857894736842, recall 0.815, Fscore 0.835897435897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8578947368421053, 0.815, 0.8358974358974358)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in pos and neg:\n",
    "import os\n",
    "import math\n",
    "pos_path = 'review_polarity/txt_norm/pos/'\n",
    "neg_path = 'review_polarity/txt_norm/neg/'\n",
    "\n",
    "pos_training = get_text(pos_path, \"cv[01234]*.txt\")\n",
    "neg_training = get_text(neg_path, \"cv[01234]*.txt\")\n",
    "train_dict = naive_bayes_train(pos_training, neg_training)\n",
    "\n",
    "textlist = get_textlist(pos_path, \"cv[67]*.txt\")+get_textlist(neg_path, \"cv[67]*.txt\")\n",
    "actual_class = ['positive']*200 + ['negative']*200\n",
    "result = naive_bayes_validation(train_dict, textlist, actual_class)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train on cv0&1&2&3&4&5 and test on cv6&7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision 0.865284974093, recall 0.835, Fscore 0.849872773537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8652849740932642, 0.835, 0.8498727735368956)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_training = get_text(pos_path, \"cv[012345]*.txt\")\n",
    "neg_training = get_text(neg_path, \"cv[012345]*.txt\")\n",
    "train_dict = naive_bayes_train(pos_training, neg_training)\n",
    "\n",
    "textlist = get_textlist(pos_path, \"cv[67]*.txt\")+get_textlist(neg_path, \"cv[67]*.txt\")\n",
    "actual_class = ['positive']*200 + ['negative']*200\n",
    "result = naive_bayes_validation(train_dict, textlist, actual_class)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "The precision, recall and Fscore is increasing after the size of training set gets bigger, but it decelerate very fast, so the marginal increase in training set size is very limit after training on cv0-5. So it's a fair judgement that the classifier is not goinging to be improved with even more training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Evaluation on cv8&9\n",
    "### train on cv0&1&2&3&4&5&6&7 and test on cv8&9\n",
    "The Fscore and precision as well as recall is lower than last training results.\n",
    "\n",
    "This comes from 2 possible reasons:\n",
    "1. The sampling of cv8 and cv9 is biased, so the distribution of words are substantially different.\n",
    "2. There are overfitting issues in the training phrase. With more training data, more features are created which include more noises into the underlying structure of the model, the real features are diluted. So the model does not generalize well in the testing data and brings down precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision 0.837837837838, recall 0.775, Fscore 0.805194805195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8378378378378378, 0.775, 0.8051948051948051)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_training = get_text(pos_path, \"cv[01234567]*.txt\")\n",
    "neg_training = get_text(neg_path, \"cv[01234567]*.txt\")\n",
    "train_dict = naive_bayes_train(pos_training, neg_training)\n",
    "\n",
    "textlist = get_textlist(pos_path, \"cv[89]*.txt\")+get_textlist(neg_path, \"cv[89]*.txt\")\n",
    "actual_class = ['positive']*200 + ['negative']*200\n",
    "result = naive_bayes_validation(train_dict, textlist, actual_class)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv800_12368.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv801_25228.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv802_28664.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv803_8207.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv804_10862.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv805_19601.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv806_8842.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv807_21740.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv808_12635.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv809_5009.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv810_12458.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv811_21386.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv812_17924.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv813_6534.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv814_18975.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv815_22456.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv816_13655.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv817_4041.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv818_10211.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv819_9364.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv820_22892.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv821_29364.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv822_20049.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv823_15569.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv824_8838.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv825_5063.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv826_11834.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv827_18331.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv828_19831.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>review_polarity/txt_norm/pos/cv829_20289.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv970_19532.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv971_11790.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv972_26837.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv973_10171.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv974_24303.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv975_11920.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv976_10724.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv977_4776.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv978_22192.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv979_2029.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv980_11851.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv981_16679.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv982_22209.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv983_24219.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv984_14006.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv985_5964.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv986_15092.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv987_7394.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv988_20168.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv989_17297.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv990_12443.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv991_19973.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv992_12806.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv993_29565.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv994_13229.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv995_23113.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv996_12447.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv997_5152.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv998_15691.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>review_polarity/txt_norm/neg/cv999_14636.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted    actual                                      filename\n",
       "0    positive  positive  review_polarity/txt_norm/pos/cv800_12368.txt\n",
       "1    positive  positive  review_polarity/txt_norm/pos/cv801_25228.txt\n",
       "2    positive  positive  review_polarity/txt_norm/pos/cv802_28664.txt\n",
       "3    positive  positive   review_polarity/txt_norm/pos/cv803_8207.txt\n",
       "4    positive  positive  review_polarity/txt_norm/pos/cv804_10862.txt\n",
       "5    positive  positive  review_polarity/txt_norm/pos/cv805_19601.txt\n",
       "6    positive  positive   review_polarity/txt_norm/pos/cv806_8842.txt\n",
       "7    positive  positive  review_polarity/txt_norm/pos/cv807_21740.txt\n",
       "8    positive  positive  review_polarity/txt_norm/pos/cv808_12635.txt\n",
       "9    positive  positive   review_polarity/txt_norm/pos/cv809_5009.txt\n",
       "10   positive  positive  review_polarity/txt_norm/pos/cv810_12458.txt\n",
       "11   positive  positive  review_polarity/txt_norm/pos/cv811_21386.txt\n",
       "12   positive  positive  review_polarity/txt_norm/pos/cv812_17924.txt\n",
       "13   positive  positive   review_polarity/txt_norm/pos/cv813_6534.txt\n",
       "14   positive  positive  review_polarity/txt_norm/pos/cv814_18975.txt\n",
       "15   negative  positive  review_polarity/txt_norm/pos/cv815_22456.txt\n",
       "16   negative  positive  review_polarity/txt_norm/pos/cv816_13655.txt\n",
       "17   positive  positive   review_polarity/txt_norm/pos/cv817_4041.txt\n",
       "18   positive  positive  review_polarity/txt_norm/pos/cv818_10211.txt\n",
       "19   negative  positive   review_polarity/txt_norm/pos/cv819_9364.txt\n",
       "20   positive  positive  review_polarity/txt_norm/pos/cv820_22892.txt\n",
       "21   positive  positive  review_polarity/txt_norm/pos/cv821_29364.txt\n",
       "22   positive  positive  review_polarity/txt_norm/pos/cv822_20049.txt\n",
       "23   positive  positive  review_polarity/txt_norm/pos/cv823_15569.txt\n",
       "24   positive  positive   review_polarity/txt_norm/pos/cv824_8838.txt\n",
       "25   negative  positive   review_polarity/txt_norm/pos/cv825_5063.txt\n",
       "26   positive  positive  review_polarity/txt_norm/pos/cv826_11834.txt\n",
       "27   negative  positive  review_polarity/txt_norm/pos/cv827_18331.txt\n",
       "28   positive  positive  review_polarity/txt_norm/pos/cv828_19831.txt\n",
       "29   positive  positive  review_polarity/txt_norm/pos/cv829_20289.txt\n",
       "..        ...       ...                                           ...\n",
       "370  negative  negative  review_polarity/txt_norm/neg/cv970_19532.txt\n",
       "371  negative  negative  review_polarity/txt_norm/neg/cv971_11790.txt\n",
       "372  positive  negative  review_polarity/txt_norm/neg/cv972_26837.txt\n",
       "373  negative  negative  review_polarity/txt_norm/neg/cv973_10171.txt\n",
       "374  negative  negative  review_polarity/txt_norm/neg/cv974_24303.txt\n",
       "375  negative  negative  review_polarity/txt_norm/neg/cv975_11920.txt\n",
       "376  negative  negative  review_polarity/txt_norm/neg/cv976_10724.txt\n",
       "377  negative  negative   review_polarity/txt_norm/neg/cv977_4776.txt\n",
       "378  positive  negative  review_polarity/txt_norm/neg/cv978_22192.txt\n",
       "379  negative  negative   review_polarity/txt_norm/neg/cv979_2029.txt\n",
       "380  negative  negative  review_polarity/txt_norm/neg/cv980_11851.txt\n",
       "381  negative  negative  review_polarity/txt_norm/neg/cv981_16679.txt\n",
       "382  negative  negative  review_polarity/txt_norm/neg/cv982_22209.txt\n",
       "383  negative  negative  review_polarity/txt_norm/neg/cv983_24219.txt\n",
       "384  negative  negative  review_polarity/txt_norm/neg/cv984_14006.txt\n",
       "385  negative  negative   review_polarity/txt_norm/neg/cv985_5964.txt\n",
       "386  negative  negative  review_polarity/txt_norm/neg/cv986_15092.txt\n",
       "387  negative  negative   review_polarity/txt_norm/neg/cv987_7394.txt\n",
       "388  negative  negative  review_polarity/txt_norm/neg/cv988_20168.txt\n",
       "389  negative  negative  review_polarity/txt_norm/neg/cv989_17297.txt\n",
       "390  negative  negative  review_polarity/txt_norm/neg/cv990_12443.txt\n",
       "391  negative  negative  review_polarity/txt_norm/neg/cv991_19973.txt\n",
       "392  negative  negative  review_polarity/txt_norm/neg/cv992_12806.txt\n",
       "393  negative  negative  review_polarity/txt_norm/neg/cv993_29565.txt\n",
       "394  negative  negative  review_polarity/txt_norm/neg/cv994_13229.txt\n",
       "395  negative  negative  review_polarity/txt_norm/neg/cv995_23113.txt\n",
       "396  negative  negative  review_polarity/txt_norm/neg/cv996_12447.txt\n",
       "397  negative  negative   review_polarity/txt_norm/neg/cv997_5152.txt\n",
       "398  negative  negative  review_polarity/txt_norm/neg/cv998_15691.txt\n",
       "399  negative  negative  review_polarity/txt_norm/neg/cv999_14636.txt\n",
       "\n",
       "[400 rows x 3 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = [naive_bayes_test(train_dict, test.split()) for test in textlist]\n",
    "result_class = pd.DataFrame(zip(predict, actual_class))\n",
    "result_class.columns = ['predicted','actual']\n",
    "result_class['filename'] = glob.glob(os.path.join(pos_path, \"cv[89]*.txt\")) + glob.glob(os.path.join(neg_path, \"cv[89]*.txt\"))\n",
    "result_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
